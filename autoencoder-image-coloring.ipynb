{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Imports\nimport numpy as np\nimport pandas as pd \nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, Input,Conv2D, Flatten, Reshape, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\nimport random\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loading","metadata":{}},{"cell_type":"code","source":"#List of image category\nlist_dir = []\nbase_dir = \"../input/food41/images\"\nfor i in os.listdir(base_dir):\n    i = os.path.join(base_dir, i)\n    list_dir.append(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = []\n# Getting all the image paths\nfor l in list_dir[:2]:\n    for i in os.listdir(l):\n        path = os.path.join(l, i)\n        image_path.append(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Randomly Shuffling all the image path\nrandom.shuffle(image_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loding and checking the size of a image\nimg = cv2.imread(image_path[0])\nplt.imshow(img)\nprint(img.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading all the images\ny = []\nfor p in image_path:\n    img= Image.open(p)\n    img= img.resize((256, 256)) #Resizing the images\n    img= np.array(img) # Converting images to array\n    img = img / 255.0 # Normalizing images for faster conversion\n    y.append(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting image to Black and White\nx = []\nfor p in image_path:\n    img= Image.open(p)\n    img= img.convert(\"L\") # Converting Image to greayscale\n    img= img.resize((256, 256))\n    img= np.array(img)\n    img = img / 255.0\n    x.append(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the lists to array\nx = np.array(x)\nx= x.reshape(x.shape[0], x.shape[1], x.shape[2], 1)\ny = np.array(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seperating train and test data\nseperation = 1500\nx_train = x[:seperation]\ny_train = y[:seperation]\nx_test = x[seperation:]\ny_test = y[seperation:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Building","metadata":{}},{"cell_type":"code","source":"# Hyperperameters\ncnn_filters = [32, 32, 64, 64,128]\ninput_shape = (256, 256, 1)\nbatch_size = 64\nkernel_size = 3\nlatent_dim =  512","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_encoder(input_shape, cnn_filters, kernel_size, latent_dim):\n    '''\n    Building the  encoder part of the model. The Encoder has 5 Conv2D layes having filter size of 32, 32, 64, 64, 128, one Flatten layer and one Dense Layer.\n    The task of the encoder is to encode the image into a  latent dimension space. From which the decoder will try to regenerate the output image.\n    '''\n    inputs = Input(input_shape, name='encoder_input')\n    x = inputs\n    for filters in cnn_filters:\n        x = Conv2D(filters=filters,\n                   kernel_size=kernel_size,\n                   strides=2,\n                   activation='relu',\n                   padding='same')(x)\n        \n    shape = K.int_shape(x)\n    x= Flatten()(x)\n    latent_space = Dense(latent_dim, name='latent_vector')(x)\n    encoder= Model(inputs, latent_space, name='encoder')\n    encoder.summary()\n    return encoder, shape, inputs","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:28.808982Z","iopub.execute_input":"2021-11-11T11:56:28.809946Z","iopub.status.idle":"2021-11-11T11:56:28.820826Z","shell.execute_reply.started":"2021-11-11T11:56:28.809898Z","shell.execute_reply":"2021-11-11T11:56:28.819507Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"def build_decoder(cnn_filters, kernel_size, latent_dim, shape, channels=3):\n    '''\n    Building the  decoder part of the model. The Decoder has 6 Conv2DTranspose layes having filter size of  128, 64, 64, 32, 32, 3 , \n    One Dense Layer and a Reshape Layer. The task of the decoder is to recreate  the image from  a  latent \n    dimension space and will try to regenerate the desired output image.\n    '''\n    latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n    x = Dense(shape[1]*shape[2]*shape[3])(latent_inputs)\n    x = Reshape((shape[1], shape[2], shape[3]))(x)\n    for filters in cnn_filters[::-1]:\n        x = Conv2DTranspose(filters=filters,\n                            kernel_size=kernel_size,\n                            strides=2,\n                            activation='relu',padding='same')(x)\n        outputs = Conv2DTranspose(filters=channels,\n                                  kernel_size=kernel_size, activation='sigmoid',\n                                  padding='same',name='decoder_output')(x)\n        \n    decoder = Model(latent_inputs, outputs, name='decoder')\n    decoder.summary()\n    return decoder","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:28.822615Z","iopub.execute_input":"2021-11-11T11:56:28.823948Z","iopub.status.idle":"2021-11-11T11:56:28.835686Z","shell.execute_reply.started":"2021-11-11T11:56:28.823855Z","shell.execute_reply":"2021-11-11T11:56:28.834399Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"def build_autoencoder(encoder, decoder, inputs):\n    '''\n    autoencoder = encoder + decoder\n    '''\n    autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n    autoencoder.summary()\n    return autoencoder","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:28.837586Z","iopub.execute_input":"2021-11-11T11:56:28.838843Z","iopub.status.idle":"2021-11-11T11:56:28.848026Z","shell.execute_reply.started":"2021-11-11T11:56:28.838778Z","shell.execute_reply":"2021-11-11T11:56:28.846790Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"encoder, shape, inputs = build_encoder(input_shape, cnn_filters, kernel_size, latent_dim)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:28.852525Z","iopub.execute_input":"2021-11-11T11:56:28.853203Z","iopub.status.idle":"2021-11-11T11:56:28.933762Z","shell.execute_reply.started":"2021-11-11T11:56:28.853152Z","shell.execute_reply":"2021-11-11T11:56:28.932822Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"decoder = build_decoder(cnn_filters, kernel_size, latent_dim, shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:28.936040Z","iopub.execute_input":"2021-11-11T11:56:28.936270Z","iopub.status.idle":"2021-11-11T11:56:29.140226Z","shell.execute_reply.started":"2021-11-11T11:56:28.936241Z","shell.execute_reply":"2021-11-11T11:56:29.139174Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"autoencoder = build_autoencoder(encoder, decoder, inputs)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:29.141994Z","iopub.execute_input":"2021-11-11T11:56:29.142402Z","iopub.status.idle":"2021-11-11T11:56:29.268354Z","shell.execute_reply.started":"2021-11-11T11:56:29.142357Z","shell.execute_reply":"2021-11-11T11:56:29.266364Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"plot_model(autoencoder, \"auto.png\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:29.270042Z","iopub.execute_input":"2021-11-11T11:56:29.271264Z","iopub.status.idle":"2021-11-11T11:56:29.675589Z","shell.execute_reply.started":"2021-11-11T11:56:29.271215Z","shell.execute_reply":"2021-11-11T11:56:29.674365Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"# Learning Rate Scheduler\nlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n                               cooldown=0,\n                               patience=3,\n                              verbose=1)\n\n# Stopping early if the model is not improving\nes_cb= EarlyStopping(monitor='val_loss', patience=5, verbose=1,)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:29.678816Z","iopub.execute_input":"2021-11-11T11:56:29.679199Z","iopub.status.idle":"2021-11-11T11:56:29.686169Z","shell.execute_reply.started":"2021-11-11T11:56:29.679144Z","shell.execute_reply":"2021-11-11T11:56:29.684535Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"autoencoder.compile(loss='mse', optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:29.688440Z","iopub.execute_input":"2021-11-11T11:56:29.688960Z","iopub.status.idle":"2021-11-11T11:56:29.710737Z","shell.execute_reply.started":"2021-11-11T11:56:29.688914Z","shell.execute_reply":"2021-11-11T11:56:29.709616Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"# Model Training\nhistory = autoencoder.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size , epochs= 50, callbacks=[lr_reducer, es_cb])","metadata":{"execution":{"iopub.status.busy":"2021-11-11T11:56:29.713245Z","iopub.execute_input":"2021-11-11T11:56:29.713742Z","iopub.status.idle":"2021-11-11T11:58:18.072423Z","shell.execute_reply.started":"2021-11-11T11:56:29.713691Z","shell.execute_reply":"2021-11-11T11:58:18.070597Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"# Plotting the graph\nplt.plot(history.history[\"val_loss\"], label= \"val_loss\")\nplt.plot(history.history[\"loss\"], label= \"val_loss\")\nplt.legend()\nplt.savefig(\"loss.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:07:09.771704Z","iopub.execute_input":"2021-11-11T12:07:09.772368Z","iopub.status.idle":"2021-11-11T12:07:10.109195Z","shell.execute_reply.started":"2021-11-11T12:07:09.772327Z","shell.execute_reply":"2021-11-11T12:07:10.108190Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"#Checking the output\ntrial_x = x_test[140]\ntrial_y = y_test[140]","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:03:49.513880Z","iopub.execute_input":"2021-11-11T12:03:49.514190Z","iopub.status.idle":"2021-11-11T12:03:49.521498Z","shell.execute_reply.started":"2021-11-11T12:03:49.514156Z","shell.execute_reply":"2021-11-11T12:03:49.520447Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"# Reshaping the data to fit in the model to make prediction\ntest_image = trial_x.reshape((1, 256, 256, 1))","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:03:50.812892Z","iopub.execute_input":"2021-11-11T12:03:50.813261Z","iopub.status.idle":"2021-11-11T12:03:50.818917Z","shell.execute_reply.started":"2021-11-11T12:03:50.813231Z","shell.execute_reply":"2021-11-11T12:03:50.817614Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"# Making the prediction\npred = autoencoder.predict(test_image)","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:03:51.112825Z","iopub.execute_input":"2021-11-11T12:03:51.113109Z","iopub.status.idle":"2021-11-11T12:03:51.173921Z","shell.execute_reply.started":"2021-11-11T12:03:51.113079Z","shell.execute_reply":"2021-11-11T12:03:51.172929Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"#Actual Image\nplt.imshow(trial_y, cmap=\"gray\")\nplt.savefig(\"true.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:03:51.345240Z","iopub.execute_input":"2021-11-11T12:03:51.345473Z","iopub.status.idle":"2021-11-11T12:03:51.697919Z","shell.execute_reply.started":"2021-11-11T12:03:51.345445Z","shell.execute_reply":"2021-11-11T12:03:51.696769Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"# Merging the predicted and input Image\npred = pred+ test_image","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:03:51.700220Z","iopub.execute_input":"2021-11-11T12:03:51.700854Z","iopub.status.idle":"2021-11-11T12:03:51.706437Z","shell.execute_reply.started":"2021-11-11T12:03:51.700806Z","shell.execute_reply":"2021-11-11T12:03:51.705283Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"# Predicted Result\nplt.imshow(pred[0])\nplt.savefig(\"pred.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:03:51.708153Z","iopub.execute_input":"2021-11-11T12:03:51.708782Z","iopub.status.idle":"2021-11-11T12:03:52.041559Z","shell.execute_reply.started":"2021-11-11T12:03:51.708704Z","shell.execute_reply":"2021-11-11T12:03:52.040582Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"# Input Image\nplt.imshow(trial_x, cmap=\"gray\")\nplt.savefig(\"input.pdf\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:03:53.556368Z","iopub.execute_input":"2021-11-11T12:03:53.556787Z","iopub.status.idle":"2021-11-11T12:03:54.394793Z","shell.execute_reply.started":"2021-11-11T12:03:53.556753Z","shell.execute_reply":"2021-11-11T12:03:54.393626Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"# Saving the model\nautoencoder.save(\"colorization.h5\")","metadata":{"execution":{"iopub.status.busy":"2021-11-11T12:05:54.031222Z","iopub.execute_input":"2021-11-11T12:05:54.031932Z","iopub.status.idle":"2021-11-11T12:05:54.367386Z","shell.execute_reply.started":"2021-11-11T12:05:54.031880Z","shell.execute_reply":"2021-11-11T12:05:54.366034Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}